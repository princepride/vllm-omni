# Stage config for running Bagel (AR only)
# Stage 0: Thinker (multimodal understanding + text generation)

stage_args:
  - stage_id: 0
    runtime:
      devices: "0"  # Adjust as needed
      max_batch_size: 1
    engine_args:
      model_stage: thinker
      model_arch: BagelForConditionalGeneration
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.4
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: text
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      tensor_parallel_size: 1
      omni_config:
        need_send_cache: true
    final_output: true
    final_output_type: text
    is_comprehension: true
    default_sampling_params:
      temperature: 0.4
      top_p: 0.9
      top_k: 1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.05

  - stage_id: 1
    stage_type: diffusion
    runtime:
      devices: "1"
      max_batch_size: 1
    engine_args:
      model_stage: dit
      model_arch: BagelForConditionalGeneration # Using the same model class wrapper or pipeline
      # Need to check what class vllm_omni expects.
      # Looking at test.py, it uses OmniDiffusion (pipeline wrapper).
      # The worker for diffusion usually wraps a pipeline.
      worker_cls: vllm_omni.worker.gpu_generation_worker.GPUGenerationWorker
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      gpu_memory_utilization: 0.4
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: image
      distributed_executor_backend: "mp"
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      tensor_parallel_size: 1
    engine_input_source: [0]
    # We need a custom transformer to convert AR output (KV) to Diffusion input (Request)
    # The user says "pass KV cache... directly... skipping logic".
    # vllm-omni usually has a `stage_input_processors` module.
    # We need to ensure the processor passes `past_key_values` in the request.

    final_output: true
    final_output_type: image
    is_comprehension: false
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.0

# Runtime edges
runtime:
  enabled: true
  defaults:
    window_size: -1
    max_inflight: 1

  edges:
    - from: 0
      to: 1
      window_size: -1
